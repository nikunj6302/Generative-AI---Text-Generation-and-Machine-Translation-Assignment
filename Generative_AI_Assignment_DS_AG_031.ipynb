{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc34b04",
   "metadata": {},
   "source": [
    "# Generative AI â€“ Text Generation and Machine Translation\n",
    "## Assignment DS-AG-031"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771dee9e",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**What is Generative AI and what are its primary use cases across industries?**\n",
    "\n",
    "Generative AI is a class of artificial intelligence models that learn patterns from existing data and generate new content such as text, images, audio, video, and code. Unlike traditional AI systems that focus on prediction or classification, generative AI can create original outputs.\n",
    "\n",
    "**Use cases:** Healthcare (medical reports, drug discovery), Education (content creation, tutoring), Media (story writing, music), Business (chatbots, marketing), Software Development (code generation), Finance (reports, simulations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d793b69",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Role of probabilistic modeling in generative models and difference from discriminative models**\n",
    "\n",
    "Generative models use probabilistic modeling to learn the underlying data distribution. They estimate how likely data points are and can generate new samples. Generative models learn joint probability P(X, Y) or P(X), while discriminative models learn conditional probability P(Y|X). Generative models can create data, whereas discriminative models focus on prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d8e09",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "**Difference between Autoencoders and Variational Autoencoders (VAEs)**\n",
    "\n",
    "Autoencoders compress data into a fixed latent representation and reconstruct it deterministically. VAEs learn probabilistic latent distributions, enabling smooth sampling and better text generation. VAEs include KL divergence for regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd3918",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "**Attention mechanisms in Neural Machine Translation**\n",
    "\n",
    "Attention allows the model to focus on relevant parts of the input while generating each output word. It improves translation accuracy, handles long sentences, and forms the basis of Transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03b272",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "**Ethical considerations in generative AI for creative content**\n",
    "\n",
    "Key concerns include authorship, plagiarism, bias, misinformation, and transparency. Responsible use requires ethical guidelines, bias mitigation, and disclosure of AI-generated content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be766f1",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "**VAE for text reconstruction (Code Implementation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"the sky is blue\",\n",
    "    \"the sun is bright\",\n",
    "    \"the grass is green\",\n",
    "    \"the night is dark\",\n",
    "    \"the stars are shining\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(max_len,))\n",
    "x = tf.keras.layers.Embedding(vocab_size, 16)(encoder_inputs)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Dense(16 * max_len, activation='relu')(z)\n",
    "decoder_outputs = tf.keras.layers.Dense(max_len, activation='linear')(decoder_inputs)\n",
    "\n",
    "vae = tf.keras.Model(encoder_inputs, decoder_outputs)\n",
    "vae.compile(optimizer='adam', loss='mse')\n",
    "vae.fit(X, X, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f3277",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "**GPT-based Translation Example**\n",
    "\n",
    "Original: The sky is blue and the sun is bright.\n",
    "\n",
    "French: Le ciel est bleu et le soleil est brillant.\n",
    "\n",
    "German: Der Himmel ist blau und die Sonne ist hell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf7a59",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "**Attention-based Encoder-Decoder (Conceptual Code)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae78d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "encoder = tf.keras.layers.LSTM(128, return_sequences=True)\n",
    "decoder = tf.keras.layers.LSTM(128)\n",
    "attention = tf.keras.layers.Attention()\n",
    "\n",
    "# Conceptual demonstration for attention-based translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003dece1",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "**Poem Generation using GPT-style Prompting**\n",
    "\n",
    "Generated Poem:\n",
    "\n",
    "The stars whisper softly at night,\n",
    "Moonlight dances in silver hue,\n",
    "Dreams float gently with the breeze,\n",
    "Wrapped in silence, calm and true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb5533",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "**Designing a Creative Writing Assistant**\n",
    "\n",
    "A creative writing assistant can be built using GPT-like models trained on diverse literary data. Key aspects include bias mitigation, evaluation through human feedback, ethical safeguards, and scalability challenges."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
